

# Anatomy of the MapReduce Job for Word Count

Hadoop runs a MapReduce job by dividing it into tasks of which there are two types: map tasks and reduce tasks. The MapReduce framework views the input to the job as a set of <key, value> pairs and produces a set of <key, value> tuples as the output of the job. In this tutorial, we will walk through the word count application to get a flavor for how MapReduce actually works, and execute the job on a fully-distributed Hadoop cluster.


## Overview

As explained in our lecture, WordCount is an application that counts the number of occurrences of each word in a given collection of documents.

The MapReduce framework first divides the input to a MapReduce job into fixed-size pieces called input splits. To do so, the Job Client checks with the NameNode the meta information (block size, data length, block locations, etc.) for the data located in the input path on the HDFS, and computes input splits based on it. These computed input splits carry meta information (e.g., it could be represented by a <input-file-path, start, offset> tuple that provides the reference to data) to the tasks to compute on. Then the framework copies the resources needed to run the job, including the job JAR file, the configuration file, and the computed input splits, to the shared filesystem (HDFS). Then the MapReduce framework retrieves the input splits computed by the client from the shared file system and spawns one map task for each input split based on data-locality (first preference), rack-locality or resource availability (memory, CPU availability).

Behind the scene, the InputFormat defines how these input files are split up and read. The input split has defined a slice of work, but does not describe how to access it. The RecordReader class actually loads the data from its source and converts it into (key, value) pairs suitable for reading by the Mapper. The RecordReader instance is also defined by the InputFormat. TextInputFormat is the default InputFormat for plain text files. It provides a LineRecordReader to break files into lines. Either line feed or carriage-return are used to signal end of line. Each record is a line of input. The key, a LongWritable, is the byte offset within the file of the beginning of the line (for simplicity, here we use the Object class which is the root of the class hierarchy). The value is the contents of the line, excluding any line terminators (e.g., newline or carriage return), and is packaged as a Text object. The RecordReader is invoke repeatedly on the input until the entire input split has been consumed. So, a file containing the following text:

> On the top of the Crumpetty Tree
The Quangle Wangle sat,
But his face you could not see,
On account of his Beaver Hat.

is divided into one split of four records. The records are interpreted as the following key-value pairs:

```
(0, On the top of the Crumpetty Tree)
(33, The Quangle Wangle sat,)
(57, But his face you could not see,)
(89, On account of his Beaver Hat.)
```


The Hadoop MapReduce framework spawns one map task for each input split generated by the InputFormat for the job. Each map task then runs the user-defined map function for each record (one line) in the split.  

The Mapper outputs are partitioned per Reducer, and copied across the network to the nodes where the Reducers are running. Upon arrival at these nodes, intermediate results from different Mappers are merged at each node and sorted them by key before being presented to a Reducer. A Reducer executes user-provided code that performs the second phase of job-specific work. For each key in the partition assigned to a Reducer, the Reducer's `reduce()` method is called once (it receives a key as well as an iterator over all the values associated with the key). Reducers then aggregate the set of values with shared key to a smaller set of values and write the output to RecordWriters.



## Source Code Details

### TokenizerMapper


```java
  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }
```  

The **Mapper class** is a generic type defined by **org.apache.hadoop.mapreduce.Mapper**. In the Java language, classes can be derived from other classes, thereby inheriting fields and methods from those classes. For more information, see [inheritance](https://docs.oracle.com/javase/tutorial/java/IandI/subclasses.html). For this word-count job, we define a subclass of the Mapper class with four formal type parameters that specify the types of the input key, input value, output key, and output value of a map task. The `extends` keyword declares that the **TokenizerMapper** class is the subclass of the **Mapper** class.

Because keys and values used in MapReduce need to be transmitted over the network and written to disks, the key and value objects have to be serializable. However, the standard Java method of serializing a Java object does not meet the requirements of Hadoop, asking Hadoop for its own set of basic types optimized for serialization<sup><a href="#footnote1">1</a></sup>. Specifically, objects marshaled to or from files and across the network in Hadoop must obey a particular [interface](https://docs.oracle.com/javase/tutorial/java/concepts/interface.html), called [**Writable**](https://hadoop.apache.org/docs/current/api/index.html). Hadoop provides several stock classes which implement Writable: Text (which stores String data), IntWritable, LongWritable, FloatWritable, BooleanWritable, and several others. The entire list is in the **org.apache.hadoop.io** package of the Hadoop source (see the [API reference](https://hadoop.apache.org/docs/current/api/index.html)).


As a result, rather than using built-in Java types, we use Text (like Java String) as the input value and the output key type, and IntWritable (like Java Integer) as the output value type, as suggested by the code below (Class Object is the root of the class hierarchy. Every class has Object as a superclass).

header??<sup><a href="#footnote2">2</a></sup><sup><a href="#footnote3">3</a></sup>

```java
public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>
{
  ...
}
```

We declare two new fields in the **TokenizerMapper** class that are not in the **Mapper** class. We define word as a Text object (since we are just using it as a key), and one as an IntWritable object with 1 wrapped in it. The private modifier specifies that the two members of the TokenizerMapper class can only be accessed in this class.

```java
private final static IntWritable one = new IntWritable(1);
private Text word = new Text();
```


The `map()` method has to be overridden in the TokenizerMapper class for counting words. It will be called for each record (a.k.a. each input key/value pair) in an input split (i.e., one line as provided by TextInputFormat). A method call breaks a record into words and emits a new key/value pair of the word and 1. It also provides an instance of Context to write the output to.

public void map(Object key, Text value, Context context) throws IOException, InterruptedException{
  StringTokenizer itr = new StringTokenizer(value.toString());
  while (itr.hasMoreTokens()) {
    word.set(itr.nextToken());
    context.write(word, one);
    }
}





<sup>[1](#footnote1)</sup> Serialization is the process of converting structured objects into a binary stream and reconstructed later in the same or another computer environment. It is done basically for one, transmission over a network, and for the other, writing to persistent storage. When the resulting series of bits is reread according to the serialization format, it can be used to create a semantically identical clone of the original object. This opposite operation, extracting a data structure from a series of bytes, is deserialization.

<sup>[2](#footnote2)</sup>We see two sequential keywords, i.e., `public static`, preceding the name of the TokenizerMapper class. Either word has its own meaning and purpose. `public` is a type of access modifiers. A class declared with this modifier is visible to all classes everywhere. `static` indicates that the TokenizerMapper class as an inner class of the WordCount class can be accessed without creating the instance of the class. For more information, see [nested classes](https://docs.oracle.com/javase/tutorial/java/javaOO/nested.html) and [access control](https://docs.oracle.com/javase/tutorial/java/javaOO/accesscontrol.html).

<sup>[3](#footnote3)</sup>The angular brackets `<>` after a class name are used to indicate [generics](http://docs.oracle.com/javase/tutorial/java/generics/index.html) in Java.  If we define `ArrayList<String>`, it means we can only add String type object to an ArrayList object.
